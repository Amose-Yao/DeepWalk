{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import argparse\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import os\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--cls_dim'], dest='cls_dim', nargs=None, const=None, default=273, type=<class 'float'>, choices=None, help='p_cls', metavar=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='video features to LSTM Language Model')\n",
    "\n",
    "# Location of data\n",
    "parser.add_argument('--dataset', type=str, default='ActivityNet',\n",
    "                    help='Name of the data class to use from data.py')\n",
    "parser.add_argument('--data', type=str, default='data/ActivityNet/activity_net.v1-3.min.json',\n",
    "                    help='location of the dataset')\n",
    "parser.add_argument('--features', type=str, default=r'G:/machine_learning/activitynet/sub_activitynet_v1-3.c3d.hdf5',\n",
    "                    help='location of the video features')\n",
    "parser.add_argument('--labels', type=str, default='data/ActivityNet/labels.hdf5',\n",
    "                    help='location of the proposal labels')\n",
    "parser.add_argument('--vid-ids', type=str, default='data/ActivityNet/video_ids.json',\n",
    "                    help='location of the video ids')\n",
    "parser.add_argument('--save', type=str, default='data/models/default',\n",
    "                    help='path to folder where to save the final model and log files and corpus')\n",
    "parser.add_argument('--save-every', type=int, default=1,\n",
    "                    help='Save the model every x epochs')\n",
    "parser.add_argument('--clean', dest='clean', action='store_true',\n",
    "                    help='Delete the models and the log files in the folder')\n",
    "parser.add_argument('--W', type=int, default=128,\n",
    "                    help='The rnn kernel size to use to get the proposal features')\n",
    "parser.add_argument('--K', type=int, default=64,\n",
    "                    help='Number of proposals')\n",
    "parser.add_argument('--max-W', type=int, default=256,\n",
    "                    help='maximum number of windows to return per video')\n",
    "\n",
    "# Model options\n",
    "parser.add_argument('--rnn-type', type=str, default='GRU',\n",
    "                    help='type of recurrent net (RNN_TANH, RNN_RELU, LSTM, GRU)')\n",
    "parser.add_argument('--rnn-num-layers', type=int, default=2,\n",
    "                    help='Number of layers in rnn')\n",
    "parser.add_argument('--rnn-dropout', type=int, default=0.0,\n",
    "                    help='dropout used in rnn')\n",
    "parser.add_argument('--video-dim', type=int, default=500,\n",
    "                    help='dimensions of video (C3D) features')\n",
    "parser.add_argument('--hidden-dim', type=int, default=512,\n",
    "                    help='dimensions output layer of video network')\n",
    "\n",
    "# Training options\n",
    "parser.add_argument('--lr', type=float, default=0.1,\n",
    "                    help='initial learning rate')\n",
    "parser.add_argument('--dropout', type=float, default=0.0,\n",
    "                    help='dropout between RNN layers')\n",
    "parser.add_argument('--momentum', type=float, default=0.9,\n",
    "                    help='SGD momentum')\n",
    "parser.add_argument('--weight-decay', type=float, default=0,\n",
    "                    help='SGD weight decay')\n",
    "parser.add_argument('--epochs', type=int, default=100,\n",
    "                    help='upper epoch limit')\n",
    "parser.add_argument('--batch-size', type=int, default=1,\n",
    "                    help='batch size')\n",
    "parser.add_argument('--seed', type=int, default=1111,\n",
    "                    help='random seed')\n",
    "parser.add_argument('--cuda', action='store_true',\n",
    "                    help='use CUDA')\n",
    "parser.add_argument('--log-interval', type=int, default=100, metavar='N',\n",
    "                    help='report interval')\n",
    "parser.add_argument('--debug', dest='debug', action='store_true',\n",
    "                    help='Print out debug sentences')\n",
    "parser.add_argument('--num-samples', type=int, default=None,\n",
    "                    help='Number of training samples to train with')\n",
    "parser.add_argument('--shuffle', type=int, default=1,\n",
    "                    help='whether to shuffle the data')\n",
    "parser.add_argument('--nthreads', type=int, default=0,\n",
    "                    help='number of worker threas used to load data')\n",
    "parser.add_argument('--resume', dest='resume', action='store_true',\n",
    "                    help='reload the model')\n",
    "\n",
    "# Evaluate options\n",
    "parser.add_argument('--num-vids-eval', type=int, default=500,\n",
    "                    help='Number of videos to evaluate at each pass')\n",
    "parser.add_argument('--iou-threshold', type=float, default=0.5,\n",
    "                    help='threshold above which we say something is positive')\n",
    "parser.add_argument('--num-proposals', type=int, default=None,\n",
    "                    help='number of top proposals to evaluate')\n",
    "\n",
    "\n",
    "parser.add_argument('--p_cls', type=float, default=0.3,\n",
    "                    help='p_cls')\n",
    "\n",
    "parser.add_argument('--cls_dim', type=float, default=273,\n",
    "                    help='p_cls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "args = parser.parse_args(args = [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2018.3.31 test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProposalDataset(object):\n",
    "    \"\"\"\n",
    "    All dataset parsing classes will inherit from this class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, args):\n",
    "        \"\"\"\n",
    "        args must contain the following:\n",
    "            data - the file that contains the Activity Net json data.\n",
    "            features - the location of where the PCA C3D 500D features are.\n",
    "        \"\"\"\n",
    "        assert os.path.exists(args.data)\n",
    "        assert os.path.exists(args.features)\n",
    "        self.data = json.load(open(args.data))\n",
    "        self.features = h5py.File(args.features)\n",
    "        if not os.path.exists(args.labels) or not os.path.exists(args.vid_ids):\n",
    "            self.generate_labels(args)\n",
    "        self.labels = h5py.File(args.labels)\n",
    "        self.vid_ids = json.load(open(args.vid_ids))\n",
    "\n",
    "    def generate_labels(self, args):\n",
    "        \"\"\"\n",
    "        Overwrite based on dataset used\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def iou(self, interval, featstamps, return_index=False):\n",
    "        \"\"\"\n",
    "        Measures temporal IoU\n",
    "        \"\"\"\n",
    "        start_i, end_i = interval[0], interval[1]\n",
    "        output = 0.0\n",
    "        gt_index = -1\n",
    "        for i, (start, end) in enumerate(featstamps):\n",
    "            intersection = max(0, min(end, end_i) - max(start, start_i))\n",
    "            union = min(max(end, end_i) - min(start, start_i), end - start + end_i - start_i)\n",
    "            overlap = float(intersection) / (union + 1e-8)\n",
    "            if overlap >= output:\n",
    "                output = overlap\n",
    "                gt_index = i\n",
    "        if return_index:\n",
    "            return output, gt_index\n",
    "        return output\n",
    "\n",
    "    def timestamp_to_featstamp(self, timestamp, nfeats, duration):\n",
    "        \"\"\"\n",
    "        Function to measure 1D overlap\n",
    "        Convert the timestamps to feature indices\n",
    "        \"\"\"\n",
    "        start, end = timestamp\n",
    "        start = min(int(round(start / duration * nfeats)), nfeats - 1)\n",
    "        end = max(int(round(end / duration * nfeats)), start + 1)\n",
    "        return start, end\n",
    "\n",
    "    def compute_proposals_stats(self, prop_captured):\n",
    "        \"\"\"\n",
    "        Function to compute the proportion of proposals captured during labels generation.\n",
    "        :param prop_captured: array of length nb_videos\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        nb_videos = len(prop_captured)\n",
    "        proportion = np.mean(prop_captured[prop_captured != -1])\n",
    "        nb_no_proposals = (prop_captured == -1).sum()\n",
    "        print(\"Number of videos in the dataset: {}\".format(nb_videos))\n",
    "        print(\"Proportion of videos with no proposals: {}\".format(1. * nb_no_proposals / nb_videos))\n",
    "        print(\"Proportion of action proposals captured during labels creation: {}\".format(proportion))\n",
    "\n",
    "\n",
    "class ActivityNet(ProposalDataset):\n",
    "    \"\"\"\n",
    "    ActivityNet is responsible for parsing the raw activity net dataset and converting it into a\n",
    "    format that DataSplit (defined below) can use. This level of abstraction is used so that\n",
    "    DataSplit can be used with other dataset and we would only need to write a class similar\n",
    "    to this one.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, args):\n",
    "        super(self.__class__, self).__init__(args)\n",
    "        self.durations = {}\n",
    "        self.gt_times = {}\n",
    "        self.w1 = self.vid_ids['w1']\n",
    "        for split in ['training', 'validation', 'testing']:\n",
    "            setattr(self, split + '_ids', self.vid_ids[split])\n",
    "            for video_id in self.vid_ids[split]:\n",
    "                self.durations[video_id] = self.data['database'][video_id]['duration']\n",
    "                self.gt_times[video_id] = [ann['segment'] for ann in self.data['database'][video_id]['annotations']]\n",
    "        self.cls_dim = args.cls_dim\n",
    "        \n",
    "\n",
    "    def generate_labels(self, args):\n",
    "        \"\"\"\n",
    "        Overwriting parent class to generate action proposal labels\n",
    "        \"\"\"\n",
    "        print(\"| Generating labels for action proposals\")\n",
    "        label_dataset = h5py.File(args.labels, 'w')\n",
    "        \n",
    "        #把classes onehot了\n",
    "        already_appear_list = []\n",
    "        cls_look_up_dict_emmbeding={}\n",
    "        rank_cls = 0\n",
    "        for _ in self.data['taxonomy']:\n",
    "            if _ ['nodeName'] not in already_appear_list:\n",
    "                already_appear_list.append(_ ['nodeName'])\n",
    "                cls_look_up_dict_emmbeding[_ ['nodeName']] = rank_cls\n",
    "                rank_cls+=1\n",
    "        self.cls_dim = len(cls_look_up_dict_emmbeding) +1\n",
    "        \n",
    "        \n",
    "        \n",
    "        # bar = progressbar.ProgressBar(maxval=len(list(self.data['database'].keys()))).start()\n",
    "        prop_captured = []\n",
    "        prop_pos_examples = []\n",
    "        video_ids = list(self.data['database'].keys())[:100]\n",
    "        split_ids = {'training': [], 'validation': [], 'testing': [],\n",
    "                     'w1': []}  # maybe find a better name since w1 is not a split\n",
    "        for progress, video_id in enumerate(video_ids):\n",
    "            features = self.features['v_' + video_id]['c3d_features']\n",
    "            nfeats = features.shape[0]\n",
    "            duration = self.data['database'][video_id]['duration']\n",
    "            annotations = self.data['database'][video_id]['annotations']\n",
    "            timestamps = [ann['segment'] for ann in annotations]\n",
    "            featstamps = [self.timestamp_to_featstamp(x, nfeats, duration) for x in timestamps]\n",
    "            nb_prop = len(featstamps)\n",
    "            for i in range(nb_prop):\n",
    "                if (featstamps[nb_prop - i - 1][1] - featstamps[nb_prop - i - 1][0]) > args.K / args.iou_threshold:\n",
    "                    # we discard these proposals since they will not be captured for this value of K \n",
    "                    del featstamps[nb_prop - i - 1]\n",
    "            if len(featstamps) == 0:\n",
    "                if len(timestamps) == 0:\n",
    "                    # no proposals il this video\n",
    "                    prop_captured += [-1.]\n",
    "                else:\n",
    "                    # no proposals captured in this video since all have a length above threshold\n",
    "                    prop_captured += [0.]\n",
    "                continue\n",
    "                # we keep track of the videos kept to update ids\n",
    "            split_ids[self.data['database'][video_id]['subset']] += [video_id]\n",
    "            \n",
    "            \n",
    "            gt_captured = []\n",
    "            #几个输出的lable和weight\n",
    "            #gru proposal的相关输出\n",
    "            labels_prop_yk = np.zeros((nfeats, args.K))\n",
    "            w_prop1 = np.zeros(args.K)########xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx一个视频只有一对w1，w2\n",
    "            w_prop2 = np.zeros(args.K)\n",
    "            \n",
    "            print(w_prop1)#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "            \n",
    "            #gru classifier的相关输出\n",
    "            labels_cls_yt = np.zeros((nfeats, self.cls_dim))#xxxxxxxxxxxx\n",
    "            labels_cls_yt[:,-1] = 1\n",
    "            wcls = np.ones(nfeats)\n",
    "            \n",
    "            #最后分类器的相关输出\n",
    "            label_overlap_iou = np.ones((nfeats,args.K))\n",
    " \n",
    " \n",
    "\n",
    "            for t in range(nfeats):            \n",
    "                for i in range (len(timestamps)):\n",
    "                    if timestamps[i][0]<=t<=timestamps[i][1]:#判断是不是背景\n",
    "                        print(len(cls_look_up_dict_emmbeding))\n",
    "                        labels_cls_yt[ t, cls_look_up_dict_emmbeding[self.data['database'][video_id]['annotations'][i]['label']] ]=1\n",
    "                        labels_cls_yt[t,-1]=0\n",
    "                        \n",
    "                    else:\n",
    "                        wcls[t] = args.p_cls\n",
    "                        \n",
    "                for k in range(args.K):\n",
    "                    iou, gt_index = self.iou([t - k, t + 1], featstamps, return_index=True)\n",
    "                    label_overlap_iou[t, k] = np.maximum(iou, 0.001)\n",
    "                    if iou >= args.iou_threshold:\n",
    "                        w_prop1[k] += 1\n",
    "                        labels_prop_yk[t, k] = 1\n",
    "                        gt_captured += [gt_index]\n",
    "                        count +=1\n",
    "\n",
    "                        \n",
    "                    else:\n",
    "                        w_prop2[k] += 1\n",
    "  \n",
    "            temp_w_prop1 = w_prop1/(w_prop1+w_prop2)\n",
    "            temp_w_prop2 = w_prop2/(w_prop1+w_prop2)\n",
    "            w_prop1,w_prop2 = temp_w_prop1,temp_w_prop2\n",
    "            \n",
    "                        \n",
    "            prop_captured += [1. * len(np.unique(gt_captured)) / len(timestamps)]\n",
    "            if self.data['database'][video_id]['subset'] == 'training':\n",
    "                prop_pos_examples += [np.sum(labels_prop_yk, axis=0) * 1. / nfeats]\n",
    "            \n",
    "            \n",
    "            video_labels_prop_yk = label_dataset.create_dataset(video_id+'_labels_prop_yk', (nfeats, args.K), dtype='f')\n",
    "            video_w_prop = label_dataset.create_dataset(video_id+'_w_prop', (2,args.K), dtype='f')#xxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
    "            video_labels_cls_yt = label_dataset.create_dataset(video_id+'_labels_cls_yt', (nfeats, self.cls_dim), dtype='f')\n",
    "            video_wcls = label_dataset.create_dataset(video_id+'_wcls', (nfeats,), dtype='f')\n",
    "            video_label_overlap_iou = label_dataset.create_dataset(video_id+'_labels_overlap_iou', (nfeats,args.K), dtype='f')\n",
    "\n",
    "            \n",
    "            video_labels_prop_yk[...] = labels_prop_yk\n",
    "            video_w_prop[0] = w_prop1\n",
    "            video_w_prop[1] = w_prop2\n",
    "            video_labels_cls_yt[...] = labels_cls_yt\n",
    "            video_wcls[...] = wcls\n",
    "            video_label_overlap_iou[...] = label_overlap_iou\n",
    "            \n",
    "            # bar.update(progress)\n",
    "        split_ids['w1'] = np.array(prop_pos_examples).mean(axis=0).tolist()  # this will be used to compute the loss\n",
    "        json.dump(split_ids, open(args.vid_ids, 'w'))\n",
    "        self.compute_proposals_stats(np.array(prop_captured))\n",
    "        # bar.finish()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class subSetActivityNet(ActivityNet):\n",
    "    def __init__(self, video_ids, dataset, args):\n",
    "        '''\n",
    "        video_ids :should be [dataset.training_ids ,dataset.validation_ids, dataset.testing_ids]\n",
    "        '''\n",
    "        self.video_ids = video_ids\n",
    "        self.features = dataset.features\n",
    "        self.labels = dataset.labels\n",
    "        self.durations = dataset.durations\n",
    "        self.gt_times = dataset.gt_times\n",
    "        self.cls_dim = dataset.cls_dim\n",
    "        self.num_samples = args.num_samples\n",
    "        self.W = args.W\n",
    "        self.K = args.K\n",
    "        self.max_W = args.max_W\n",
    "        \n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        video_id = self.video_ids[index]\n",
    "        features = self.features['v_' + video_id]['c3d_features']\n",
    "\n",
    "        labels_prop_yk = self.labels[video_id + '_labels_prop_yk']\n",
    "        w_prop = self.labels[video_id + '_w_prop']\n",
    "        labels_cls_yt = self.labels[video_id + '_labels_cls_yt']\n",
    "        wcls = self.labels[video_id + '_wcls']\n",
    "        label_overlap_iou = self.labels[video_id + '_labels_overlap_iou']\n",
    "\n",
    "\n",
    "        nfeats = features.shape[0]    \n",
    "        nWindows = max(1, nfeats - self.W + 1)\n",
    "\n",
    "\n",
    "        sample = list(range(nWindows))\n",
    "        if self.max_W < nWindows:\n",
    "            sample = np.random.choice(nWindows, self.max_W)\n",
    "            nWindows = self.max_W   \n",
    "\n",
    "        feature_windows = np.zeros((nWindows, self.W, features.shape[1]))\n",
    "        labels_prop_yk_windows = np.zeros((nWindows, self.W, args.K))\n",
    "        labels_cls_yt_windows = np.zeros((nWindows, self.W, self.cls_dim))\n",
    "        wcls_windows = np.zeros((nWindows, self.W))\n",
    "        label_overlap_iou_windows = np.zeros((nWindows, self.W, args.K))\n",
    "\n",
    "        for j, w_start in enumerate(sample):\n",
    "            w_end = min(w_start + self.W, nfeats)\n",
    "            feature_windows[j, 0:w_end - w_start, :] = features[w_start:w_end, :]\n",
    "            labels_prop_yk_windows[j, 0:w_end - w_start, :] = labels_prop_yk[w_start:w_end, :]\n",
    "            labels_cls_yt_windows[j, 0:w_end - w_start, :] = labels_cls_yt[w_start:w_end, :]\n",
    "            wcls_windows[j, 0:w_end - w_start] = wcls[w_start:w_end]\n",
    "            label_overlap_iou_windows[j, 0:w_end - w_start, :]=label_overlap_iou[w_start:w_end,:]\n",
    "\n",
    "\n",
    "\n",
    "#         return torch.FloatTensor(feature_windows), torch.FloatTensor(labels_prop_yk_windows),\\\n",
    "#                 torch.FloatTensor(w_prop), torch.FloatTensor(labels_cls_yt_windows), \\\n",
    "#                 torch.FloatTensor(wcls_windows), torch.FloatTensor(label_overlap_iou)\n",
    "            \n",
    "        return feature_windows[...], labels_prop_yk_windows[...],\\\n",
    "                w_prop[...], labels_cls_yt_windows[...], \\\n",
    "                wcls_windows[...], label_overlap_iou[...]          \n",
    "            \n",
    "            \n",
    "    def __len__(self):\n",
    "        if self.num_samples is not None:\n",
    "            # in case num sample is greater than the dataset itself\n",
    "            return min(self.num_samples, len(self.video_ids))\n",
    "        return len(self.video_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "aa = ActivityNet(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "273"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa.cls_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = subSetActivityNet(aa.training_ids, aa, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bb = enumerate(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "tried to construct a tensor from a nested float sequence, but found an item of type numpy.float32 at index (0, 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-831ef103667f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0maa\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mE:\\ProgramFiles\\Anaconda\\envs\\torch03\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    186\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# same-process loading\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m             \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 188\u001b[1;33m             \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    189\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\ProgramFiles\\Anaconda\\envs\\torch03\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    186\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# same-process loading\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m             \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 188\u001b[1;33m             \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    189\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-dd70a25f1f1a>\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_windows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels_prop_yk_windows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m                \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw_prop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels_cls_yt_windows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m                 \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwcls_windows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel_overlap_iou\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;31m#         return feature_windows[...], labels_prop_yk_windows[...],\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: tried to construct a tensor from a nested float sequence, but found an item of type numpy.float32 at index (0, 0)"
     ]
    }
   ],
   "source": [
    "aa = bb.__next__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.ActivityNet at 0x1f668ea57b8>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/ActivityNet/labels.hdf5'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file = h5py.File(args.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0Gr4aKQzGYk_labels_cls_yt',\n",
       " '0Gr4aKQzGYk_labels_overlap_iou',\n",
       " '0Gr4aKQzGYk_labels_prop_yk',\n",
       " '0Gr4aKQzGYk_w_prop',\n",
       " '0Gr4aKQzGYk_wcls',\n",
       " '3joaQzU05MY_labels_cls_yt',\n",
       " '3joaQzU05MY_labels_overlap_iou',\n",
       " '3joaQzU05MY_labels_prop_yk',\n",
       " '3joaQzU05MY_w_prop',\n",
       " '3joaQzU05MY_wcls',\n",
       " '5Bo0gFXxDQk_labels_cls_yt',\n",
       " '5Bo0gFXxDQk_labels_overlap_iou',\n",
       " '5Bo0gFXxDQk_labels_prop_yk',\n",
       " '5Bo0gFXxDQk_w_prop',\n",
       " '5Bo0gFXxDQk_wcls',\n",
       " '6uhLrPgbpUA_labels_cls_yt',\n",
       " '6uhLrPgbpUA_labels_overlap_iou',\n",
       " '6uhLrPgbpUA_labels_prop_yk',\n",
       " '6uhLrPgbpUA_w_prop',\n",
       " '6uhLrPgbpUA_wcls',\n",
       " '97McCuWAynA_labels_cls_yt',\n",
       " '97McCuWAynA_labels_overlap_iou',\n",
       " '97McCuWAynA_labels_prop_yk',\n",
       " '97McCuWAynA_w_prop',\n",
       " '97McCuWAynA_wcls',\n",
       " 'GCtrfXIBbwA_labels_cls_yt',\n",
       " 'GCtrfXIBbwA_labels_overlap_iou',\n",
       " 'GCtrfXIBbwA_labels_prop_yk',\n",
       " 'GCtrfXIBbwA_w_prop',\n",
       " 'GCtrfXIBbwA_wcls',\n",
       " 'IGcsVPa34Hc_labels_cls_yt',\n",
       " 'IGcsVPa34Hc_labels_overlap_iou',\n",
       " 'IGcsVPa34Hc_labels_prop_yk',\n",
       " 'IGcsVPa34Hc_w_prop',\n",
       " 'IGcsVPa34Hc_wcls',\n",
       " 'JDg--pjY5gg_labels_cls_yt',\n",
       " 'JDg--pjY5gg_labels_overlap_iou',\n",
       " 'JDg--pjY5gg_labels_prop_yk',\n",
       " 'JDg--pjY5gg_w_prop',\n",
       " 'JDg--pjY5gg_wcls',\n",
       " 'Ou24uqaFRPg_labels_cls_yt',\n",
       " 'Ou24uqaFRPg_labels_overlap_iou',\n",
       " 'Ou24uqaFRPg_labels_prop_yk',\n",
       " 'Ou24uqaFRPg_w_prop',\n",
       " 'Ou24uqaFRPg_wcls',\n",
       " 'R4ES1QLRvtg_labels_cls_yt',\n",
       " 'R4ES1QLRvtg_labels_overlap_iou',\n",
       " 'R4ES1QLRvtg_labels_prop_yk',\n",
       " 'R4ES1QLRvtg_w_prop',\n",
       " 'R4ES1QLRvtg_wcls',\n",
       " 'UkA6pgt29VI_labels_cls_yt',\n",
       " 'UkA6pgt29VI_labels_overlap_iou',\n",
       " 'UkA6pgt29VI_labels_prop_yk',\n",
       " 'UkA6pgt29VI_w_prop',\n",
       " 'UkA6pgt29VI_wcls',\n",
       " 'UvuXGKesWS0_labels_cls_yt',\n",
       " 'UvuXGKesWS0_labels_overlap_iou',\n",
       " 'UvuXGKesWS0_labels_prop_yk',\n",
       " 'UvuXGKesWS0_w_prop',\n",
       " 'UvuXGKesWS0_wcls',\n",
       " 'XnctHnlJB4g_labels_cls_yt',\n",
       " 'XnctHnlJB4g_labels_overlap_iou',\n",
       " 'XnctHnlJB4g_labels_prop_yk',\n",
       " 'XnctHnlJB4g_w_prop',\n",
       " 'XnctHnlJB4g_wcls',\n",
       " 'j4Ru2L4u0Qk_labels_cls_yt',\n",
       " 'j4Ru2L4u0Qk_labels_overlap_iou',\n",
       " 'j4Ru2L4u0Qk_labels_prop_yk',\n",
       " 'j4Ru2L4u0Qk_w_prop',\n",
       " 'j4Ru2L4u0Qk_wcls',\n",
       " 'jIQFVSymHQs_labels_cls_yt',\n",
       " 'jIQFVSymHQs_labels_overlap_iou',\n",
       " 'jIQFVSymHQs_labels_prop_yk',\n",
       " 'jIQFVSymHQs_w_prop',\n",
       " 'jIQFVSymHQs_wcls',\n",
       " 'kQ4rE7o6rrg_labels_cls_yt',\n",
       " 'kQ4rE7o6rrg_labels_overlap_iou',\n",
       " 'kQ4rE7o6rrg_labels_prop_yk',\n",
       " 'kQ4rE7o6rrg_w_prop',\n",
       " 'kQ4rE7o6rrg_wcls',\n",
       " 'lVMMPkvnid8_labels_cls_yt',\n",
       " 'lVMMPkvnid8_labels_overlap_iou',\n",
       " 'lVMMPkvnid8_labels_prop_yk',\n",
       " 'lVMMPkvnid8_w_prop',\n",
       " 'lVMMPkvnid8_wcls',\n",
       " 'lW4OZ8eP3ns_labels_cls_yt',\n",
       " 'lW4OZ8eP3ns_labels_overlap_iou',\n",
       " 'lW4OZ8eP3ns_labels_prop_yk',\n",
       " 'lW4OZ8eP3ns_w_prop',\n",
       " 'lW4OZ8eP3ns_wcls',\n",
       " 'p800u2wCKbE_labels_cls_yt',\n",
       " 'p800u2wCKbE_labels_overlap_iou',\n",
       " 'p800u2wCKbE_labels_prop_yk',\n",
       " 'p800u2wCKbE_w_prop',\n",
       " 'p800u2wCKbE_wcls',\n",
       " 'rWQz-EwA4EA_labels_cls_yt',\n",
       " 'rWQz-EwA4EA_labels_overlap_iou',\n",
       " 'rWQz-EwA4EA_labels_prop_yk',\n",
       " 'rWQz-EwA4EA_w_prop',\n",
       " 'rWQz-EwA4EA_wcls',\n",
       " 'rs7er4e67ec_labels_cls_yt',\n",
       " 'rs7er4e67ec_labels_overlap_iou',\n",
       " 'rs7er4e67ec_labels_prop_yk',\n",
       " 'rs7er4e67ec_w_prop',\n",
       " 'rs7er4e67ec_wcls',\n",
       " 'sjyZWmvTGA4_labels_cls_yt',\n",
       " 'sjyZWmvTGA4_labels_overlap_iou',\n",
       " 'sjyZWmvTGA4_labels_prop_yk',\n",
       " 'sjyZWmvTGA4_w_prop',\n",
       " 'sjyZWmvTGA4_wcls',\n",
       " 't2wawfMaTzM_labels_cls_yt',\n",
       " 't2wawfMaTzM_labels_overlap_iou',\n",
       " 't2wawfMaTzM_labels_prop_yk',\n",
       " 't2wawfMaTzM_w_prop',\n",
       " 't2wawfMaTzM_wcls',\n",
       " 'w2HnFjJei7k_labels_cls_yt',\n",
       " 'w2HnFjJei7k_labels_overlap_iou',\n",
       " 'w2HnFjJei7k_labels_prop_yk',\n",
       " 'w2HnFjJei7k_w_prop',\n",
       " 'w2HnFjJei7k_wcls',\n",
       " 'whJ6ESGNoyY_labels_cls_yt',\n",
       " 'whJ6ESGNoyY_labels_overlap_iou',\n",
       " 'whJ6ESGNoyY_labels_prop_yk',\n",
       " 'whJ6ESGNoyY_w_prop',\n",
       " 'whJ6ESGNoyY_wcls',\n",
       " 'x99PS_O6JW8_labels_cls_yt',\n",
       " 'x99PS_O6JW8_labels_overlap_iou',\n",
       " 'x99PS_O6JW8_labels_prop_yk',\n",
       " 'x99PS_O6JW8_w_prop',\n",
       " 'x99PS_O6JW8_wcls',\n",
       " 'yUSo_xjyGhM_labels_cls_yt',\n",
       " 'yUSo_xjyGhM_labels_overlap_iou',\n",
       " 'yUSo_xjyGhM_labels_prop_yk',\n",
       " 'yUSo_xjyGhM_w_prop',\n",
       " 'yUSo_xjyGhM_wcls']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[_ for _ in file.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.41509435,  0.40566039,\n",
       "         0.41509435,  0.4245283 ,  0.41509435,  0.4245283 ,  0.4245283 ,\n",
       "         0.41509435,  0.41509435,  0.41509435,  0.40566039,  0.40566039,\n",
       "         0.40566039,  0.39622641,  0.39622641,  0.39622641,  0.38679245,\n",
       "         0.38679245,  0.38679245,  0.3773585 ,  0.3773585 ],\n",
       "       [ 1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  0.58490568,  0.59433961,\n",
       "         0.58490568,  0.5754717 ,  0.58490568,  0.5754717 ,  0.5754717 ,\n",
       "         0.58490568,  0.58490568,  0.58490568,  0.59433961,  0.59433961,\n",
       "         0.59433961,  0.60377359,  0.60377359,  0.60377359,  0.61320752,\n",
       "         0.61320752,  0.61320752,  0.6226415 ,  0.6226415 ]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file['sjyZWmvTGA4_w_prop'].value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0Gr4aKQzGYk_label_overlap_iou',\n",
       " '0Gr4aKQzGYk_labels_cls_yt',\n",
       " '0Gr4aKQzGYk_labels_prop_yk',\n",
       " '0Gr4aKQzGYk_w_prop',\n",
       " '0Gr4aKQzGYk_wcls',\n",
       " '3joaQzU05MY_label_overlap_iou',\n",
       " '3joaQzU05MY_labels_cls_yt',\n",
       " '3joaQzU05MY_labels_prop_yk',\n",
       " '3joaQzU05MY_w_prop',\n",
       " '3joaQzU05MY_wcls',\n",
       " '5Bo0gFXxDQk_label_overlap_iou',\n",
       " '5Bo0gFXxDQk_labels_cls_yt',\n",
       " '5Bo0gFXxDQk_labels_prop_yk',\n",
       " '5Bo0gFXxDQk_w_prop',\n",
       " '5Bo0gFXxDQk_wcls',\n",
       " '6uhLrPgbpUA_label_overlap_iou',\n",
       " '6uhLrPgbpUA_labels_cls_yt',\n",
       " '6uhLrPgbpUA_labels_prop_yk',\n",
       " '6uhLrPgbpUA_w_prop',\n",
       " '6uhLrPgbpUA_wcls',\n",
       " '97McCuWAynA_label_overlap_iou',\n",
       " '97McCuWAynA_labels_cls_yt',\n",
       " '97McCuWAynA_labels_prop_yk',\n",
       " '97McCuWAynA_w_prop',\n",
       " '97McCuWAynA_wcls',\n",
       " 'GCtrfXIBbwA_label_overlap_iou',\n",
       " 'GCtrfXIBbwA_labels_cls_yt',\n",
       " 'GCtrfXIBbwA_labels_prop_yk',\n",
       " 'GCtrfXIBbwA_w_prop',\n",
       " 'GCtrfXIBbwA_wcls',\n",
       " 'IGcsVPa34Hc_label_overlap_iou',\n",
       " 'IGcsVPa34Hc_labels_cls_yt',\n",
       " 'IGcsVPa34Hc_labels_prop_yk',\n",
       " 'IGcsVPa34Hc_w_prop',\n",
       " 'IGcsVPa34Hc_wcls',\n",
       " 'JDg--pjY5gg_label_overlap_iou',\n",
       " 'JDg--pjY5gg_labels_cls_yt',\n",
       " 'JDg--pjY5gg_labels_prop_yk',\n",
       " 'JDg--pjY5gg_w_prop',\n",
       " 'JDg--pjY5gg_wcls',\n",
       " 'Ou24uqaFRPg_label_overlap_iou',\n",
       " 'Ou24uqaFRPg_labels_cls_yt',\n",
       " 'Ou24uqaFRPg_labels_prop_yk',\n",
       " 'Ou24uqaFRPg_w_prop',\n",
       " 'Ou24uqaFRPg_wcls',\n",
       " 'R4ES1QLRvtg_label_overlap_iou',\n",
       " 'R4ES1QLRvtg_labels_cls_yt',\n",
       " 'R4ES1QLRvtg_labels_prop_yk',\n",
       " 'R4ES1QLRvtg_w_prop',\n",
       " 'R4ES1QLRvtg_wcls',\n",
       " 'UkA6pgt29VI_label_overlap_iou',\n",
       " 'UkA6pgt29VI_labels_cls_yt',\n",
       " 'UkA6pgt29VI_labels_prop_yk',\n",
       " 'UkA6pgt29VI_w_prop',\n",
       " 'UkA6pgt29VI_wcls',\n",
       " 'UvuXGKesWS0_label_overlap_iou',\n",
       " 'UvuXGKesWS0_labels_cls_yt',\n",
       " 'UvuXGKesWS0_labels_prop_yk',\n",
       " 'UvuXGKesWS0_w_prop',\n",
       " 'UvuXGKesWS0_wcls',\n",
       " 'XnctHnlJB4g_label_overlap_iou',\n",
       " 'XnctHnlJB4g_labels_cls_yt',\n",
       " 'XnctHnlJB4g_labels_prop_yk',\n",
       " 'XnctHnlJB4g_w_prop',\n",
       " 'XnctHnlJB4g_wcls',\n",
       " 'j4Ru2L4u0Qk_label_overlap_iou',\n",
       " 'j4Ru2L4u0Qk_labels_cls_yt',\n",
       " 'j4Ru2L4u0Qk_labels_prop_yk',\n",
       " 'j4Ru2L4u0Qk_w_prop',\n",
       " 'j4Ru2L4u0Qk_wcls',\n",
       " 'jIQFVSymHQs_label_overlap_iou',\n",
       " 'jIQFVSymHQs_labels_cls_yt',\n",
       " 'jIQFVSymHQs_labels_prop_yk',\n",
       " 'jIQFVSymHQs_w_prop',\n",
       " 'jIQFVSymHQs_wcls',\n",
       " 'kQ4rE7o6rrg_label_overlap_iou',\n",
       " 'kQ4rE7o6rrg_labels_cls_yt',\n",
       " 'kQ4rE7o6rrg_labels_prop_yk',\n",
       " 'kQ4rE7o6rrg_w_prop',\n",
       " 'kQ4rE7o6rrg_wcls',\n",
       " 'lVMMPkvnid8_label_overlap_iou',\n",
       " 'lVMMPkvnid8_labels_cls_yt',\n",
       " 'lVMMPkvnid8_labels_prop_yk',\n",
       " 'lVMMPkvnid8_w_prop',\n",
       " 'lVMMPkvnid8_wcls',\n",
       " 'lW4OZ8eP3ns_label_overlap_iou',\n",
       " 'lW4OZ8eP3ns_labels_cls_yt',\n",
       " 'lW4OZ8eP3ns_labels_prop_yk',\n",
       " 'lW4OZ8eP3ns_w_prop',\n",
       " 'lW4OZ8eP3ns_wcls',\n",
       " 'p800u2wCKbE_label_overlap_iou',\n",
       " 'p800u2wCKbE_labels_cls_yt',\n",
       " 'p800u2wCKbE_labels_prop_yk',\n",
       " 'p800u2wCKbE_w_prop',\n",
       " 'p800u2wCKbE_wcls',\n",
       " 'rWQz-EwA4EA_label_overlap_iou',\n",
       " 'rWQz-EwA4EA_labels_cls_yt',\n",
       " 'rWQz-EwA4EA_labels_prop_yk',\n",
       " 'rWQz-EwA4EA_w_prop',\n",
       " 'rWQz-EwA4EA_wcls',\n",
       " 'rs7er4e67ec_label_overlap_iou',\n",
       " 'rs7er4e67ec_labels_cls_yt',\n",
       " 'rs7er4e67ec_labels_prop_yk',\n",
       " 'rs7er4e67ec_w_prop',\n",
       " 'rs7er4e67ec_wcls',\n",
       " 'sjyZWmvTGA4_label_overlap_iou',\n",
       " 'sjyZWmvTGA4_labels_cls_yt',\n",
       " 'sjyZWmvTGA4_labels_prop_yk',\n",
       " 'sjyZWmvTGA4_w_prop',\n",
       " 'sjyZWmvTGA4_wcls',\n",
       " 't2wawfMaTzM_label_overlap_iou',\n",
       " 't2wawfMaTzM_labels_cls_yt',\n",
       " 't2wawfMaTzM_labels_prop_yk',\n",
       " 't2wawfMaTzM_w_prop',\n",
       " 't2wawfMaTzM_wcls',\n",
       " 'w2HnFjJei7k_label_overlap_iou',\n",
       " 'w2HnFjJei7k_labels_cls_yt',\n",
       " 'w2HnFjJei7k_labels_prop_yk',\n",
       " 'w2HnFjJei7k_w_prop',\n",
       " 'w2HnFjJei7k_wcls',\n",
       " 'whJ6ESGNoyY_label_overlap_iou',\n",
       " 'whJ6ESGNoyY_labels_cls_yt',\n",
       " 'whJ6ESGNoyY_labels_prop_yk',\n",
       " 'whJ6ESGNoyY_w_prop',\n",
       " 'whJ6ESGNoyY_wcls',\n",
       " 'x99PS_O6JW8_label_overlap_iou',\n",
       " 'x99PS_O6JW8_labels_cls_yt',\n",
       " 'x99PS_O6JW8_labels_prop_yk',\n",
       " 'x99PS_O6JW8_w_prop',\n",
       " 'x99PS_O6JW8_wcls',\n",
       " 'yUSo_xjyGhM_label_overlap_iou',\n",
       " 'yUSo_xjyGhM_labels_cls_yt',\n",
       " 'yUSo_xjyGhM_labels_prop_yk',\n",
       " 'yUSo_xjyGhM_w_prop',\n",
       " 'yUSo_xjyGhM_wcls']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[_ for _ in aa.labels.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 file \"sub_activitynet_v1-3.c3d.hdf5\" (mode r+)>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "135"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([_ for _ in aa.labels.keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "myactivitynet = ActivityNet2(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "self = myactivitynet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_dataset = h5py.File(args.labels+'3.31', 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "        already_appear_list = []\n",
    "        cls_look_up_dict_emmbeding={}\n",
    "        rank_cls = 0\n",
    "        for _ in self.data['taxonomy']:\n",
    "            if _ ['nodeName'] not in already_appear_list:\n",
    "                already_appear_list.append(_ ['nodeName'])\n",
    "                cls_look_up_dict_emmbeding[_ ['nodeName']] = rank_cls\n",
    "                rank_cls+=1\n",
    "        cls_dim = len(cls_look_up_dict_emmbeding)+1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "272"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cls_look_up_dict_emmbeding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "273"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(519, 273)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_cls_yt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "        prop_captured = []\n",
    "        prop_pos_examples = []\n",
    "        video_ids = list(self.data['database'].keys())\n",
    "        split_ids = {'training': [], 'validation': [], 'testing': [],\n",
    "                     'w1': []} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "enumerateor_video_ids = enumerate(video_ids[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb_prop:1\n",
      "featstamps:[(0, 461)]\n",
      "need continue\n"
     ]
    }
   ],
   "source": [
    "progress, video_id = enumerateor_video_ids.__next__()\n",
    "\n",
    "features = self.features['v_' + video_id]['c3d_features']\n",
    "nfeats = features.shape[0]\n",
    "duration = self.data['database'][video_id]['duration']\n",
    "annotations = self.data['database'][video_id]['annotations']\n",
    "timestamps = [ann['segment'] for ann in annotations]\n",
    "featstamps = [self.timestamp_to_featstamp(x, nfeats, duration) for x in timestamps]\n",
    "\n",
    "nb_prop = len(featstamps)\n",
    "\n",
    "print(\"nb_prop:{}\".format(nb_prop))\n",
    "\n",
    "print('featstamps:{}'.format(featstamps))\n",
    "\n",
    "for i in range(nb_prop):\n",
    "    if (featstamps[nb_prop - i - 1][1] - featstamps[nb_prop - i - 1][0]) > args.K / args.iou_threshold:\n",
    "        # we discard these proposals since they will not be captured for this value of K \n",
    "        del featstamps[nb_prop - i - 1]\n",
    "if len(featstamps) == 0:\n",
    "    print('need continue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50.48"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[26.214528861154445, 40.54198907956318]]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gt_captured = []\n",
    "\n",
    "labels_prop_yk = np.zeros((nfeats, args.K))\n",
    "w_prop1 = w_prop2 = np.zeros(args.K)########xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx一个视频只有一对w1，w2\n",
    "\n",
    "#gru classifier的相关输出\n",
    "labels_cls_yt = np.zeros((nfeats, cls_dim))#xxxxxxxxxxxx\n",
    "labels_cls_yt[:,-1] = 1\n",
    "wcls = np.ones(nfeats)\n",
    "\n",
    "#最后分类器的相关输出\n",
    "label_overlap_iou = np.ones((nfeats,args.K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(nfeats):            \n",
    "    for i in range (len(timestamps)):\n",
    "        if timestamps[i][0]<=t<=timestamps[i][1]:#判断是不是背景\n",
    "\n",
    "            labels_cls_yt[t][ cls_look_up_dict_emmbeding[self.data['database'][video_id]['annotations'][i]['label']] ]=1\n",
    "            labels_cls_yt[t,-1]=0\n",
    "\n",
    "        else:\n",
    "            wcls[t] = args.p_cls\n",
    "\n",
    "    for k in range(args.K):\n",
    "        iou, gt_index = self.iou([t - k, t + 1], featstamps, return_index=True)\n",
    "        label_overlap_iou[t, k] = np.maximum(iou, 0.001)\n",
    "        if iou >= args.iou_threshold:\n",
    "            labels_prop_yk[t, k] = 1\n",
    "            gt_captured += [gt_index]\n",
    "\n",
    "            w_prop1[k] += 1######xxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
    "        else:\n",
    "            w_prop2[k] +=1######xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
    "\n",
    "temp_w_prop1 = w_prop1/(w_prop1+w_prop2)\n",
    "temp_w_prop2 = w_prop2/(w_prop1+w_prop2)\n",
    "w_prop1,w_prop2 = temp_w_prop1,temp_w_prop2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       ..., \n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  1.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_prop_yk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.001     ,  0.001     ,  0.001     , ...,  0.001     ,\n",
       "         0.001     ,  0.001     ],\n",
       "       [ 0.001     ,  0.001     ,  0.001     , ...,  0.001     ,\n",
       "         0.001     ,  0.001     ],\n",
       "       [ 0.001     ,  0.001     ,  0.001     , ...,  0.001     ,\n",
       "         0.001     ,  0.001     ],\n",
       "       ..., \n",
       "       [ 0.001     ,  0.001     ,  0.001     , ...,  0.48571429,\n",
       "         0.5       ,  0.51428571],\n",
       "       [ 0.001     ,  0.001     ,  0.001     , ...,  0.46478873,\n",
       "         0.47887324,  0.49295775],\n",
       "       [ 0.001     ,  0.001     ,  0.001     , ...,  0.44444444,\n",
       "         0.45833333,  0.47222222]])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_overlap_iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.3,  0.3,  0.3,  0.3,  0.3,  0.3,  0.3,  0.3,  0.3,  0.3,  0.3,\n",
       "        0.3,  0.3,  0.3,  0.3,  0.3,  0.3,  0.3,  0.3,  0.3,  0.3,  0.3,\n",
       "        0.3,  0.3,  0.3,  0.3,  0.3,  1. ,  1. ,  1. ,  1. ,  1. ,  1. ,\n",
       "        1. ,  1. ,  1. ,  1. ,  1. ,  1. ,  1. ,  1. ,  0.3,  0.3,  0.3,\n",
       "        0.3,  0.3,  0.3,  0.3,  0.3,  0.3,  0.3,  0.3,  0.3,  0.3,  0.3,\n",
       "        0.3,  0.3,  0.3,  0.3,  0.3,  0.3,  0.3,  0.3,  0.3,  0.3,  0.3,\n",
       "        0.3,  0.3,  0.3,  0.3,  0.3,  0.3,  0.3,  0.3,  0.3,  0.3,  0.3,\n",
       "        0.3,  0.3,  0.3,  0.3,  0.3,  0.3,  0.3,  0.3,  0.3,  0.3,  0.3,\n",
       "        0.3,  0.3,  0.3,  0.3,  0.3,  0.3,  0.3,  0.3,  0.3,  0.3,  0.3,\n",
       "        0.3,  0.3,  0.3,  0.3,  0.3,  0.3,  0.3,  0.3,  0.3,  0.3,  0.3,\n",
       "        0.3,  0.3,  0.3,  0.3,  0.3,  0.3,  0.3,  0.3,  0.3,  0.3,  0.3,\n",
       "        0.3,  0.3,  0.3,  0.3,  0.3,  0.3,  0.3,  0.3,  0.3,  0.3,  0.3,\n",
       "        0.3,  0.3,  0.3,  0.3,  0.3,  0.3,  0.3,  0.3,  0.3,  0.3,  0.3,\n",
       "        0.3,  0.3,  0.3,  0.3,  0.3,  0.3,  0.3])"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wcls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.0033004 ,  0.0033004 ,  0.0033004 ,  0.0033004 ,  0.0033004 ,\n",
       "        0.0033004 ,  0.0033004 ,  0.0033004 ,  0.0033004 ,  0.0033004 ,\n",
       "        0.0033004 ,  0.0033004 ,  0.0033004 ,  0.0033004 ,  0.0033004 ,\n",
       "        0.0033004 ,  0.0033004 ,  0.0033004 ,  0.0033004 ,  0.0033004 ,\n",
       "        0.0033004 ,  0.1419173 ,  0.14851811,  0.1419173 ,  0.14851811,\n",
       "        0.15511891,  0.14851811,  0.15511891,  0.16171972,  0.15511891,\n",
       "        0.16171972,  0.16832052,  0.16171972,  0.16832052,  0.17492133,\n",
       "        0.16832052,  0.17492133,  0.18152213,  0.17492133,  0.18152213,\n",
       "        0.18812294,  0.18152213,  0.18812294,  0.19472374,  0.18812294,\n",
       "        0.19472374,  0.20132455,  0.19472374,  0.20132455,  0.20792535,\n",
       "        0.20132455,  0.20792535,  0.21452616,  0.20792535,  0.21452616,\n",
       "        0.22112696,  0.21452616,  0.22112696,  0.22772777,  0.22112696,\n",
       "        0.22772777,  0.23432857,  0.22772777,  0.23432857])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_prop1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.9966996 ,  0.9966996 ,  0.9966996 ,  0.9966996 ,  0.9966996 ,\n",
       "        0.9966996 ,  0.9966996 ,  0.9966996 ,  0.9966996 ,  0.9966996 ,\n",
       "        0.9966996 ,  0.9966996 ,  0.9966996 ,  0.9966996 ,  0.9966996 ,\n",
       "        0.9966996 ,  0.9966996 ,  0.9966996 ,  0.9966996 ,  0.9966996 ,\n",
       "        0.9966996 ,  0.8580827 ,  0.85148189,  0.8580827 ,  0.85148189,\n",
       "        0.84488109,  0.85148189,  0.84488109,  0.83828028,  0.84488109,\n",
       "        0.83828028,  0.83167948,  0.83828028,  0.83167948,  0.82507867,\n",
       "        0.83167948,  0.82507867,  0.81847787,  0.82507867,  0.81847787,\n",
       "        0.81187706,  0.81847787,  0.81187706,  0.80527626,  0.81187706,\n",
       "        0.80527626,  0.79867545,  0.80527626,  0.79867545,  0.79207465,\n",
       "        0.79867545,  0.79207465,  0.78547384,  0.79207465,  0.78547384,\n",
       "        0.77887304,  0.78547384,  0.77887304,  0.77227223,  0.77887304,\n",
       "        0.77227223,  0.76567143,  0.77227223,  0.76567143])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_prop2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2018.3.31 test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 dataset \"c3d_features\": shape (150, 500), type \"<f8\">"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.max_W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aa= [1,2,3,4,5,6,7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 1])"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(4, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def __getitem__(self,idex):\n",
    "    video_id = self.video_ids[index]\n",
    "    features = self.features['v_' + video_id]['c3d_features']\n",
    "    \n",
    "    labels_prop_yk = self.labels[video_id + '_labels_prop_yk']\n",
    "    w_prop = self.labels[video_id + '_w_prop']\n",
    "    labels_cls_yt = self.labels[video_id + 'labels_cls_yt']\n",
    "    wcls = self.labels[video_id + '_wcls']\n",
    "    label_overlap_iou = self.labels[video_id + '_overlap_iou']\n",
    "    \n",
    "    \n",
    "    nfeats = features.shape[0]    \n",
    "    nWindows = max(1, nfeats - self.W + 1)\n",
    "    \n",
    "    \n",
    "    sample = list(range(nWindows))\n",
    "    if self.max_W < nWindows:\n",
    "        sample = np.random.choice(nWindows, self.max_W)\n",
    "        nWindows = self.max_W   \n",
    "        \n",
    "    feature_windows = np.zeros((nWindows, self.W, features.shape[1]))\n",
    "    labels_prop_yk_windows = np.zeros((nWindows, self.W, args.K))\n",
    "    labels_cls_yt_windows = np.zeros((nWindows, self.W, self.cls_dim))\n",
    "    wcls_windows = np.zeros((nWindows, self.W))\n",
    "    label_overlap_iou_windows = np.zeros((nWindows, self.W, args.K))\n",
    "    \n",
    "    for j, w_start in enumerate(sample):\n",
    "        w_end = min(w_start + self.W, nfeats)\n",
    "        feature_windows[j, 0:w_end - w_start, :] = features[w_start:w_end, :]\n",
    "        labels_prop_yk_windows[j, 0:w_end - w_start, :] = labels_prop_yk[w_start:w_end, :]\n",
    "        labels_cls_yt_windows[j, 0:w_end - w_start, :] = labels_cls_yt[w_start:w_end, :]\n",
    "        wcls_windows[j, 0:w_end - w_start] = wcls[w_start:w_end]\n",
    "        label_overlap_iou_windows[j, 0:w_end - w_start, :]=label_overlap_iou[w_start:w_end,:]\n",
    "\n",
    "    \n",
    "    \n",
    "    return torch.FloatTensor(feature_windows), torch.FloatTensor(labels_prop_yk_windows),\\\n",
    "            torch.FloatTensor(w_prop), torch.FloatTensor(labels_cls_yt_windows), \\\n",
    "            torch.FloatTensor(wcls_windows), torch.FloatTensor(label_overlap_iou)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 64)"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_overlap_iou.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nWindows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_windows = np.zeros((nWindows, self.W, features.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23, 128, 500)"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_windows.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_enumerate = enumerate(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "j, w_start  = sample_enumerate.__next__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w_end = min(w_start + self.W, nfeats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_windows[j, 0:w_end - w_start, :] = features[w_start:w_end, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-243-17de48e73390>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-243-17de48e73390>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    a = /\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "a = /\n",
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-247-cd8aca8d5385>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-247-cd8aca8d5385>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    a = 1   2    3\u001b[0m\n\u001b[1;37m            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "a = (1,\\\n",
    "    2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
